{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Darwin SDK and CLI Official documentation of darwin-py , for managing datasets and annotations on V7 Darwin . Typical use cases for Darwin include: Create/remove/list datasets Upload/download data to/from remote datasets Convert between annotations formats Direct integration with PyTorch DataLoaders (See torch ) Darwin-py can both be used from the command line and as a python library . Installation pip install darwin-py Once installed, darwin will be available from the command line. Note : darwin-py has been tested for python >= 3.6, while older versions might work they are not supported. PyTorch To use Darwin's PyTorch dataloaders the torch extra package is needed: pip install darwin-py[torch]","title":"Darwin SDK and CLI"},{"location":"#darwin-sdk-and-cli","text":"Official documentation of darwin-py , for managing datasets and annotations on V7 Darwin . Typical use cases for Darwin include: Create/remove/list datasets Upload/download data to/from remote datasets Convert between annotations formats Direct integration with PyTorch DataLoaders (See torch ) Darwin-py can both be used from the command line and as a python library .","title":"Darwin SDK and CLI"},{"location":"#installation","text":"pip install darwin-py Once installed, darwin will be available from the command line. Note : darwin-py has been tested for python >= 3.6, while older versions might work they are not supported.","title":"Installation"},{"location":"#pytorch","text":"To use Darwin's PyTorch dataloaders the torch extra package is needed: pip install darwin-py[torch]","title":"PyTorch"},{"location":"commandline/","text":"Command line Authentication To perform remote operations on Darwin you first need to authenticate. Authentication requires a team-specific API-key . If you do not already have a Darwin account, you can contact us and we can set one up for you. To start the authentication process: $ darwin authenticate API key: *********** Make example-team the default team? [y/N] y Datasets directory [~/.darwin/datasets]: Authentication succeeded. You will be then prompted to enter your API-key, whether you want to set the corresponding team as default and finally the desired location on the local file system for the datasets of that team. This process will create a configuration file at ~/.darwin/config.yaml . This file will be updated with future authentications for different teams. Note that the API key rights selected when requesting the key determine which of the following commands are allowed. If the key has insufficient permissions for an action an error will be shown Insufficient permissions or Invalid API key . Datasets A central part of Darwin are the datasets. They contain images/videos, annotation classes, instructions and annotations. In this documentation we will refer to a dataset as remote when it's hosted on Darwin, and local when a copy has been made locally. A dataset can either be referred by its name directly, but this can lead to ambiguity when authenticating with multiple teams (hence the use of default team), or by team-name/dataset-name . Team and Dataset names are slugified: lower case spaces is replaced with dash special characters are stripped Datasets can generate a release by freezing all annotations made up to a certain point in time, this release will have a unique name using this format team-name/dataset-name:version-name See (Releases)[###Releases] Creating a dataset Creates a new remote dataset (hosted on Darwin): $ darwin dataset create test Dataset 'mydataset' (example-team/mydataset) has been created. Access at https://darwin.v7labs.com/datasets/579 List remote datasets Lists all datasets for the default team $ darwin dataset remote NAME IMAGES PROGRESS example-team/mydataset 112025 73.0% example-team/fg-removal 3000 42.0% Use the -t flag to specify a different team $ darwin dataset remote -t other-team NAME IMAGES PROGRESS other-team/ct-scans 450 100.0% List local datasets Local datasets are datasets that have been downloaded, $ darwin dataset local NAME IMAGES SYNC_DATE SIZE example-team/mydataset 999 May 25 150.2 MB other-team/ct-scans 450 May 20 500.1 MB Uploading images/videos For images and videos $ darwin dataset push my-team/test /path/to/folder/with/images 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00, 1.27it/s] Specifying framerate for videos $ darwin dataset push my-team/test --fps 2 my_video.mp4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00, 1.27it/s] The -e/--exclude argument allows to indicate file extension/s to be ignored from the data_dir. e.g.: -e .jpg For videos, the frame rate extraction rate can be specified by adding --fps <frame_rate> Supported extensions: Video files: [ .mp4 , .bpm , .mov formats]. Image files [ .jpg , .jpeg , .png formats]. Releases To download data from a dataset, a release first needs to be created. Several releases can be kept locally at the same time, images are shared between the releases to reduce disk usage. /dataset_directory # by default ~/.darwin/datasets /team-name /dataset-name /images # shared between all releases 1.png 2.png 3.png /releases /latest # symlink to the latest release (v3) /v3 1.json 2.json 3.json /v2 1.json 2.json /v1 1.json Create a new release $ darwin dataset export test 0.1 Dataset test successfully exported to example-team/test:0.1 List all releases $ darwin dataset releases example-team/my-new-dataset NAME IMAGES CLASSES EXPORT_DATE example-team/my-new-dataset:0.4 23 9 2020-05-11 14:48:25+00:00 example-team/my-new-dataset:0.2 1 8 2020-04-28 23:50:56+00:00 example-team/my-new-dataset:0.1 22 7 2020-04-17 09:57:22+00:00 Download a release $ darwin dataset pull example-team/my-new-dataset:0.1 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:20<00:00, 7.18it/s] Dataset example-team/my-new-dataset:0.4 downloaded at ~/.darwin/datasets/example-team Note that if this was the latest release for this dataset, then this release can also be referred to it via :latest for local operations. Path to dataset A convenient command to find a dataset on the local filesystem (if downloaded) $ darwin dataset path example-team/my-new-dataset /Users/me/.darwin/datasets/example-team/my-new-dataset Delete a remote dataset This goes without saying, but be careful when deleting datasets. Their data will be cleared from Darwin's servers and unrecoverable after a couple of days. $ darwin dataset remove test About to delete example-team/test on darwin. Do you want to continue? [y/N] y Import annotations If you want to bootstrap your dataset by importing already existing annotations, first make sure that all the images are already uploaded. Then ensure that the annotations are in one of the following formats [PascalVoc, COCO, CSV Tags]. $ darwin dataset import example-team/test pascal_voc sample_voc.xml Fetching remote file list... Fetching remote class list... Retrieving local annotations ... 1 annotation file(s) found. 1 file(s) are missing from the dataset imports/sample_voc.xml: '12.png' Do you want to continue? [y/N] y 0 classes are missing remotely. There are few things going on here: first darwin-py tries to match each file in the import against files in the dataset, if that fails it will warn us and allow the operation to be cancelled. then annotation classes are matched (exact name + type). If they are missing remotely darwin-py will prompt asking if it's allowed to create them. finally all the annotations are uploaded. Note In the current version, old annotations are deleted before the imports are applied. Convert annotations $ darwin dataset convert v7/my-new-dataset:standard coco output_directory This converts the downloaded annotations from the darwin format to an external format. Currently supported: [COCO, CVAT, Pascal VOC] Note Some annotations types are not valid for some formats, when that happens the annotation is simply dropped.","title":"Command line"},{"location":"commandline/#command-line","text":"","title":"Command line"},{"location":"commandline/#authentication","text":"To perform remote operations on Darwin you first need to authenticate. Authentication requires a team-specific API-key . If you do not already have a Darwin account, you can contact us and we can set one up for you. To start the authentication process: $ darwin authenticate API key: *********** Make example-team the default team? [y/N] y Datasets directory [~/.darwin/datasets]: Authentication succeeded. You will be then prompted to enter your API-key, whether you want to set the corresponding team as default and finally the desired location on the local file system for the datasets of that team. This process will create a configuration file at ~/.darwin/config.yaml . This file will be updated with future authentications for different teams. Note that the API key rights selected when requesting the key determine which of the following commands are allowed. If the key has insufficient permissions for an action an error will be shown Insufficient permissions or Invalid API key .","title":"Authentication"},{"location":"commandline/#datasets","text":"A central part of Darwin are the datasets. They contain images/videos, annotation classes, instructions and annotations. In this documentation we will refer to a dataset as remote when it's hosted on Darwin, and local when a copy has been made locally. A dataset can either be referred by its name directly, but this can lead to ambiguity when authenticating with multiple teams (hence the use of default team), or by team-name/dataset-name . Team and Dataset names are slugified: lower case spaces is replaced with dash special characters are stripped Datasets can generate a release by freezing all annotations made up to a certain point in time, this release will have a unique name using this format team-name/dataset-name:version-name See (Releases)[###Releases]","title":"Datasets"},{"location":"commandline/#creating-a-dataset","text":"Creates a new remote dataset (hosted on Darwin): $ darwin dataset create test Dataset 'mydataset' (example-team/mydataset) has been created. Access at https://darwin.v7labs.com/datasets/579","title":"Creating a dataset"},{"location":"commandline/#list-remote-datasets","text":"Lists all datasets for the default team $ darwin dataset remote NAME IMAGES PROGRESS example-team/mydataset 112025 73.0% example-team/fg-removal 3000 42.0% Use the -t flag to specify a different team $ darwin dataset remote -t other-team NAME IMAGES PROGRESS other-team/ct-scans 450 100.0%","title":"List remote datasets"},{"location":"commandline/#list-local-datasets","text":"Local datasets are datasets that have been downloaded, $ darwin dataset local NAME IMAGES SYNC_DATE SIZE example-team/mydataset 999 May 25 150.2 MB other-team/ct-scans 450 May 20 500.1 MB","title":"List local datasets"},{"location":"commandline/#uploading-imagesvideos","text":"For images and videos $ darwin dataset push my-team/test /path/to/folder/with/images 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00, 1.27it/s] Specifying framerate for videos $ darwin dataset push my-team/test --fps 2 my_video.mp4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00, 1.27it/s] The -e/--exclude argument allows to indicate file extension/s to be ignored from the data_dir. e.g.: -e .jpg For videos, the frame rate extraction rate can be specified by adding --fps <frame_rate> Supported extensions: Video files: [ .mp4 , .bpm , .mov formats]. Image files [ .jpg , .jpeg , .png formats].","title":"Uploading images/videos"},{"location":"commandline/#releases","text":"To download data from a dataset, a release first needs to be created. Several releases can be kept locally at the same time, images are shared between the releases to reduce disk usage. /dataset_directory # by default ~/.darwin/datasets /team-name /dataset-name /images # shared between all releases 1.png 2.png 3.png /releases /latest # symlink to the latest release (v3) /v3 1.json 2.json 3.json /v2 1.json 2.json /v1 1.json","title":"Releases"},{"location":"commandline/#create-a-new-release","text":"$ darwin dataset export test 0.1 Dataset test successfully exported to example-team/test:0.1","title":"Create a new release"},{"location":"commandline/#list-all-releases","text":"$ darwin dataset releases example-team/my-new-dataset NAME IMAGES CLASSES EXPORT_DATE example-team/my-new-dataset:0.4 23 9 2020-05-11 14:48:25+00:00 example-team/my-new-dataset:0.2 1 8 2020-04-28 23:50:56+00:00 example-team/my-new-dataset:0.1 22 7 2020-04-17 09:57:22+00:00","title":"List all releases"},{"location":"commandline/#download-a-release","text":"$ darwin dataset pull example-team/my-new-dataset:0.1 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:20<00:00, 7.18it/s] Dataset example-team/my-new-dataset:0.4 downloaded at ~/.darwin/datasets/example-team Note that if this was the latest release for this dataset, then this release can also be referred to it via :latest for local operations.","title":"Download a release"},{"location":"commandline/#path-to-dataset","text":"A convenient command to find a dataset on the local filesystem (if downloaded) $ darwin dataset path example-team/my-new-dataset /Users/me/.darwin/datasets/example-team/my-new-dataset","title":"Path to dataset"},{"location":"commandline/#delete-a-remote-dataset","text":"This goes without saying, but be careful when deleting datasets. Their data will be cleared from Darwin's servers and unrecoverable after a couple of days. $ darwin dataset remove test About to delete example-team/test on darwin. Do you want to continue? [y/N] y","title":"Delete a remote dataset"},{"location":"commandline/#import-annotations","text":"If you want to bootstrap your dataset by importing already existing annotations, first make sure that all the images are already uploaded. Then ensure that the annotations are in one of the following formats [PascalVoc, COCO, CSV Tags]. $ darwin dataset import example-team/test pascal_voc sample_voc.xml Fetching remote file list... Fetching remote class list... Retrieving local annotations ... 1 annotation file(s) found. 1 file(s) are missing from the dataset imports/sample_voc.xml: '12.png' Do you want to continue? [y/N] y 0 classes are missing remotely. There are few things going on here: first darwin-py tries to match each file in the import against files in the dataset, if that fails it will warn us and allow the operation to be cancelled. then annotation classes are matched (exact name + type). If they are missing remotely darwin-py will prompt asking if it's allowed to create them. finally all the annotations are uploaded. Note In the current version, old annotations are deleted before the imports are applied.","title":"Import annotations"},{"location":"commandline/#convert-annotations","text":"$ darwin dataset convert v7/my-new-dataset:standard coco output_directory This converts the downloaded annotations from the darwin format to an external format. Currently supported: [COCO, CVAT, Pascal VOC] Note Some annotations types are not valid for some formats, when that happens the annotation is simply dropped.","title":"Convert annotations"},{"location":"library/","text":"Library A central concept to darwin-py is the client object. It is authenticated with an API key and then used to create a Dataset Object (high level access) or diretly communicate with Darwin (low level access). Simple example client = Client.local() # use the configuration in ~/.darwin/config.yaml dataset = client.get_remote_dataset(\"example-team/test\") dataset.pull() # downloads annotations and images for the latest exported version Creating the client There are two ways of creating a Client object, directly from an API key from darwin.client import Client client = Client.from_api_key(\"DHMhAWr.BHucps-tKMAi6rWF1xieOpUvNe5WzrHP\") or via the local configuration ~/.darwin/config.yaml (see command line authentication ) from darwin.client import Client client = Client.local() If the API key is invalid (malformed or archived) a darwin.exceptions.InvalidLogin will be raised. Find a remote dataset dataset = client.get_remote_dataset(\"example-team/test\") If the team isn't valid, darwin.exceptions.InvalidTeam will be raised. If the dataset cannot be accessed, darwin.exception.NotFound will be raised.","title":"Library"},{"location":"library/#library","text":"A central concept to darwin-py is the client object. It is authenticated with an API key and then used to create a Dataset Object (high level access) or diretly communicate with Darwin (low level access). Simple example client = Client.local() # use the configuration in ~/.darwin/config.yaml dataset = client.get_remote_dataset(\"example-team/test\") dataset.pull() # downloads annotations and images for the latest exported version","title":"Library"},{"location":"library/#creating-the-client","text":"There are two ways of creating a Client object, directly from an API key from darwin.client import Client client = Client.from_api_key(\"DHMhAWr.BHucps-tKMAi6rWF1xieOpUvNe5WzrHP\") or via the local configuration ~/.darwin/config.yaml (see command line authentication ) from darwin.client import Client client = Client.local() If the API key is invalid (malformed or archived) a darwin.exceptions.InvalidLogin will be raised.","title":"Creating the client"},{"location":"library/#find-a-remote-dataset","text":"dataset = client.get_remote_dataset(\"example-team/test\") If the team isn't valid, darwin.exceptions.InvalidTeam will be raised. If the dataset cannot be accessed, darwin.exception.NotFound will be raised.","title":"Find a remote dataset"},{"location":"torch/","text":"PyTorch bindings Loading a dataset in Python This module includes some functionality to import your datasets ready to be plugged into your Pytorch-based libraries. For this, you can use the function get_dataset : get_dataset(dataset_slug, dataset_type [, partition, split, split_type, transform]) Input ---------- dataset_slug: str Slug of the dataset to retrieve dataset_type: str The type of dataset [classification, instance-segmentation, semantic-segmentation] partition: str Selects one of the partitions [train, val, test, None]. (Default: None) split: str Selects the split that defines the percentages used. (Default: 'default') split_type: str Heuristic used to do the split [random, stratified]. (Default: 'random') transform : list[torchvision.transforms] List of PyTorch transforms. (Default: None) Output ---------- dataset: LocalDataset API class to the local dataset Note: For now, it only support three types of dataset: classification , instance-segmentation , and semantic-segmentation . These different modes use different API classes, which load and pre-process the data in different ways, suited for these specific tasks. If you need a different API or a different pre-processing for a different task you can take a look into the implementation of these APIs in darwin.torch.dataset and extend LocalDataset in the way it suits your needs best. This is an example of how to load the v7-demo/bird-species dataset ready to be used in a instance segmentation task by using \"instance-segmentation\" as dataset_type . First, we will pull it from Darwin using darwin-py 's CLI, and will create train, validation, and test partitions: darwin dataset pull v7-demo/bird-species darwin dataset split v7-demo/bird-species --val-percentage 10 --test-percentage 20 Once downloaded, we can use get_dataset to load the different partitions, and pass different transformations for train and validation splits (some basic transformations are implemented in darwin.torch.transforms but you can also use your own transformations): from darwin.torch import get_dataset import darwin.torch.transforms as T dataset_slug = \"v7-demo/bird-species\" trfs_train = T.Compose([T.RandomHorizontalFlip(), T.ToTensor()]) db_train = get_dataset(dataset_slug, dataset_type=\"instance-segmentation\", \\ partition=\"train\", split_type=\"stratified\", transform=trfs_train) trfs_val = T.ToTensor() db_val = get_dataset(dataset_slug, dataset_type=\"instance-segmentation\", \\ partition=\"val\", split_type=\"stratified\", transform=trfs_val) print(db_train) # Returns: # InstanceSegmentationDataset(): # Root: /datasets/v7-demo/bird-species # Number of images: 1336 # Number of classes: 3 Darwin \u2715 Torchvision This tutorial shows how to train an instance segmentation model on a Darwin dataset using Pytorch's Torchvsion and darwin-py . If you do not have Pytorch and Torchvision installed yet, you can follow these installation instructions . First, using darwin-py 's CLI, we will pull the dataset from Darwin and create train, validation, and test partitions. darwin dataset pull v7-demo/bird-species darwin dataset split v7-demo/bird-species --val-percentage 10 --test-percentage 20 Now, in Python, we will start by importing some torchvision and darwin-py functions, and by defining the function get_instance_segmentation_model that we will use to instantiate a Mask-RCNN model using Torchvision's API. import torch import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor from darwin.torch import get_dataset import darwin.torch.transforms as T def collate_fn(batch): return tuple(zip(*batch)) def get_instance_segmentation_model(num_classes): # load an instance segmentation model pre-trained on COCO model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) # add a new bounding box predictor in_features = model.roi_heads.box_predictor.cls_score.in_features model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # add a new mask predictor in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes) return model Then, we will load the dataset using darwin-py 's get_dataset function, specifying the dataset slug, the dataset type (in this case we need an instance-segmentation dataset), and the train partition. The dataset that we get back can be used directly into Pytorch's standard DataLoader. trfs_train = T.Compose([T.RandomHorizontalFlip(), T.ToTensor()]) dataset = get_dataset(\"v7-demo/bird-species\", dataset_type=\"instance-segmentation\", partition=\"train\", split_type=\"stratified\", transform=trfs_train) data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn) Next, we instantiate the instance segmentation model and define the optimizer and the learning rate scheduler. device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # get the model using our helper function num_classes = dataset.num_classes + 1 # number of classes in the dataset + background model = get_instance_segmentation_model(num_classes) model.to(device) # construct an optimizer params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) And finally, we write our training loop and train the model for 10 full epochs. # let's train it for 10 epochs for epoch in range(10): # train for one epoch, printing every 10 iterations acumm_loss = 0 for i, (images, targets) in enumerate(data_loader): images = list(image.to(device) for image in images) targets = [{k: v.to(device) for k, v in t.items() if isinstance(v, torch.Tensor)} for t in targets] loss_dict = model(images, targets) losses = sum(loss for loss in loss_dict.values()) optimizer.zero_grad() losses.backward() optimizer.step() acumm_loss += losses.cpu().item() if i % 10 == 0: print(f\"Loss: {acumm_loss/10}\") acumm_loss = 0 lr_scheduler.step() Darwin \u2715 Detectron2 This tutorial shows how to train Detectron2 models in your Darwin datasets. If you do not have Detectron2 installed yet, please follow these installation instructions . Detectron2 organizes the datasets in DatasetCatalog , so the only thing we will need to do is to register our Darwin dataset in this catalog. For this, darwin-py provides the function detectron2_register_dataset , which takes the following parameters: detectron2_register_dataset(dataset_slug [, partition, split, split_type, release_name, evaluator_type]) Input ---------- dataset_slug: Path, str Slug of the dataset you want to register partition: str Selects one of the partitions [train, val, test]. If None, loads the whole dataset. (default: None) split Selects the split that defines the percetages used (use 'default' to select the default split) split_type: str Heuristic used to do the split [random, stratified] (default: stratified) release_name: str Version of the dataset. If None, takes the latest (default: None) evaluator_type: str Evaluator to be used in the val and test sets (default: None) Output ---------- catalog_name: str Name used to register this dataset partition in DatasetCatalog Here's an example of how to use this function to register a Darwin dataset, and train an instance segmentation model on it. First, and as we did before, we will start by pulling the dataset from Darwin and splitting it into train and validation from the command line: darwin dataset pull v7-demo/bird-species darwin dataset split v7-demo/bird-species --val-percentage 10 --test-percentage 20 Now, in Python, we will import some detectron2 utils and we will register the Darwin dataset into Detectron2's catalog. import os # import some common Detectron2 and Darwin utilities from detectron2.utils.logger import setup_logger from detectron2 import model_zoo from detectron2.engine import DefaultTrainer from detectron2.config import get_cfg from detectron2.data import MetadataCatalog, build_detection_test_loader from detectron2.evaluation import COCOEvaluator, inference_on_dataset from darwin.torch.utils import detectron2_register_dataset # Register both training and validation sets dataset_slug = 'v7-demo/bird-species' dataset_train = detectron2_register_dataset(dataset_slug, partition='train', split_type='stratified') dataset_val = detectron2_register_dataset(dataset_slug, partition='val', split_type='stratified') Next, we will set up the model and the training configuration, and launch the training. # Set up training configuration and train the model cfg = get_cfg() cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")) cfg.DATASETS.TRAIN = (dataset_train,) cfg.DATASETS.TEST = () cfg.DATALOADER.NUM_WORKERS = 2 cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") cfg.SOLVER.IMS_PER_BATCH = 8 cfg.SOLVER.BASE_LR = 0.005 # pick a good LR cfg.SOLVER.MAX_ITER = 1000 # and a good number of iterations cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(MetadataCatalog.get(dataset_train).thing_classes) # Instantiate the trainer and train the model os.makedirs(cfg.OUTPUT_DIR, exist_ok=True) trainer = DefaultTrainer(cfg) trainer.resume_or_load(resume=False) setup_logger() trainer.train() Finally, we will evaluate the model using the built-in COCO evaluator. # Evaluate the model cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\") cfg.DATASETS.TEST = (dataset_val, ) evaluator = COCOEvaluator(dataset_val, cfg, False, output_dir=\"./output/\") val_loader = build_detection_test_loader(cfg, dataset_val) inference_on_dataset(trainer.model, val_loader, evaluator)","title":"PyTorch bindings"},{"location":"torch/#pytorch-bindings","text":"","title":"PyTorch bindings"},{"location":"torch/#loading-a-dataset-in-python","text":"This module includes some functionality to import your datasets ready to be plugged into your Pytorch-based libraries. For this, you can use the function get_dataset : get_dataset(dataset_slug, dataset_type [, partition, split, split_type, transform]) Input ---------- dataset_slug: str Slug of the dataset to retrieve dataset_type: str The type of dataset [classification, instance-segmentation, semantic-segmentation] partition: str Selects one of the partitions [train, val, test, None]. (Default: None) split: str Selects the split that defines the percentages used. (Default: 'default') split_type: str Heuristic used to do the split [random, stratified]. (Default: 'random') transform : list[torchvision.transforms] List of PyTorch transforms. (Default: None) Output ---------- dataset: LocalDataset API class to the local dataset Note: For now, it only support three types of dataset: classification , instance-segmentation , and semantic-segmentation . These different modes use different API classes, which load and pre-process the data in different ways, suited for these specific tasks. If you need a different API or a different pre-processing for a different task you can take a look into the implementation of these APIs in darwin.torch.dataset and extend LocalDataset in the way it suits your needs best. This is an example of how to load the v7-demo/bird-species dataset ready to be used in a instance segmentation task by using \"instance-segmentation\" as dataset_type . First, we will pull it from Darwin using darwin-py 's CLI, and will create train, validation, and test partitions: darwin dataset pull v7-demo/bird-species darwin dataset split v7-demo/bird-species --val-percentage 10 --test-percentage 20 Once downloaded, we can use get_dataset to load the different partitions, and pass different transformations for train and validation splits (some basic transformations are implemented in darwin.torch.transforms but you can also use your own transformations): from darwin.torch import get_dataset import darwin.torch.transforms as T dataset_slug = \"v7-demo/bird-species\" trfs_train = T.Compose([T.RandomHorizontalFlip(), T.ToTensor()]) db_train = get_dataset(dataset_slug, dataset_type=\"instance-segmentation\", \\ partition=\"train\", split_type=\"stratified\", transform=trfs_train) trfs_val = T.ToTensor() db_val = get_dataset(dataset_slug, dataset_type=\"instance-segmentation\", \\ partition=\"val\", split_type=\"stratified\", transform=trfs_val) print(db_train) # Returns: # InstanceSegmentationDataset(): # Root: /datasets/v7-demo/bird-species # Number of images: 1336 # Number of classes: 3","title":"Loading a dataset in Python"},{"location":"torch/#darwin-torchvision","text":"This tutorial shows how to train an instance segmentation model on a Darwin dataset using Pytorch's Torchvsion and darwin-py . If you do not have Pytorch and Torchvision installed yet, you can follow these installation instructions . First, using darwin-py 's CLI, we will pull the dataset from Darwin and create train, validation, and test partitions. darwin dataset pull v7-demo/bird-species darwin dataset split v7-demo/bird-species --val-percentage 10 --test-percentage 20 Now, in Python, we will start by importing some torchvision and darwin-py functions, and by defining the function get_instance_segmentation_model that we will use to instantiate a Mask-RCNN model using Torchvision's API. import torch import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor from darwin.torch import get_dataset import darwin.torch.transforms as T def collate_fn(batch): return tuple(zip(*batch)) def get_instance_segmentation_model(num_classes): # load an instance segmentation model pre-trained on COCO model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) # add a new bounding box predictor in_features = model.roi_heads.box_predictor.cls_score.in_features model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # add a new mask predictor in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes) return model Then, we will load the dataset using darwin-py 's get_dataset function, specifying the dataset slug, the dataset type (in this case we need an instance-segmentation dataset), and the train partition. The dataset that we get back can be used directly into Pytorch's standard DataLoader. trfs_train = T.Compose([T.RandomHorizontalFlip(), T.ToTensor()]) dataset = get_dataset(\"v7-demo/bird-species\", dataset_type=\"instance-segmentation\", partition=\"train\", split_type=\"stratified\", transform=trfs_train) data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn) Next, we instantiate the instance segmentation model and define the optimizer and the learning rate scheduler. device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # get the model using our helper function num_classes = dataset.num_classes + 1 # number of classes in the dataset + background model = get_instance_segmentation_model(num_classes) model.to(device) # construct an optimizer params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) And finally, we write our training loop and train the model for 10 full epochs. # let's train it for 10 epochs for epoch in range(10): # train for one epoch, printing every 10 iterations acumm_loss = 0 for i, (images, targets) in enumerate(data_loader): images = list(image.to(device) for image in images) targets = [{k: v.to(device) for k, v in t.items() if isinstance(v, torch.Tensor)} for t in targets] loss_dict = model(images, targets) losses = sum(loss for loss in loss_dict.values()) optimizer.zero_grad() losses.backward() optimizer.step() acumm_loss += losses.cpu().item() if i % 10 == 0: print(f\"Loss: {acumm_loss/10}\") acumm_loss = 0 lr_scheduler.step()","title":"Darwin &#x2715; Torchvision"},{"location":"torch/#darwin-detectron2","text":"This tutorial shows how to train Detectron2 models in your Darwin datasets. If you do not have Detectron2 installed yet, please follow these installation instructions . Detectron2 organizes the datasets in DatasetCatalog , so the only thing we will need to do is to register our Darwin dataset in this catalog. For this, darwin-py provides the function detectron2_register_dataset , which takes the following parameters: detectron2_register_dataset(dataset_slug [, partition, split, split_type, release_name, evaluator_type]) Input ---------- dataset_slug: Path, str Slug of the dataset you want to register partition: str Selects one of the partitions [train, val, test]. If None, loads the whole dataset. (default: None) split Selects the split that defines the percetages used (use 'default' to select the default split) split_type: str Heuristic used to do the split [random, stratified] (default: stratified) release_name: str Version of the dataset. If None, takes the latest (default: None) evaluator_type: str Evaluator to be used in the val and test sets (default: None) Output ---------- catalog_name: str Name used to register this dataset partition in DatasetCatalog Here's an example of how to use this function to register a Darwin dataset, and train an instance segmentation model on it. First, and as we did before, we will start by pulling the dataset from Darwin and splitting it into train and validation from the command line: darwin dataset pull v7-demo/bird-species darwin dataset split v7-demo/bird-species --val-percentage 10 --test-percentage 20 Now, in Python, we will import some detectron2 utils and we will register the Darwin dataset into Detectron2's catalog. import os # import some common Detectron2 and Darwin utilities from detectron2.utils.logger import setup_logger from detectron2 import model_zoo from detectron2.engine import DefaultTrainer from detectron2.config import get_cfg from detectron2.data import MetadataCatalog, build_detection_test_loader from detectron2.evaluation import COCOEvaluator, inference_on_dataset from darwin.torch.utils import detectron2_register_dataset # Register both training and validation sets dataset_slug = 'v7-demo/bird-species' dataset_train = detectron2_register_dataset(dataset_slug, partition='train', split_type='stratified') dataset_val = detectron2_register_dataset(dataset_slug, partition='val', split_type='stratified') Next, we will set up the model and the training configuration, and launch the training. # Set up training configuration and train the model cfg = get_cfg() cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")) cfg.DATASETS.TRAIN = (dataset_train,) cfg.DATASETS.TEST = () cfg.DATALOADER.NUM_WORKERS = 2 cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") cfg.SOLVER.IMS_PER_BATCH = 8 cfg.SOLVER.BASE_LR = 0.005 # pick a good LR cfg.SOLVER.MAX_ITER = 1000 # and a good number of iterations cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(MetadataCatalog.get(dataset_train).thing_classes) # Instantiate the trainer and train the model os.makedirs(cfg.OUTPUT_DIR, exist_ok=True) trainer = DefaultTrainer(cfg) trainer.resume_or_load(resume=False) setup_logger() trainer.train() Finally, we will evaluate the model using the built-in COCO evaluator. # Evaluate the model cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\") cfg.DATASETS.TEST = (dataset_val, ) evaluator = COCOEvaluator(dataset_val, cfg, False, output_dir=\"./output/\") val_loader = build_detection_test_loader(cfg, dataset_val) inference_on_dataset(trainer.model, val_loader, evaluator)","title":"Darwin &#x2715; Detectron2"}]}